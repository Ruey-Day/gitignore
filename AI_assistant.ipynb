{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "12XcNtowEP-2lWTTOM3IoebsdsrXt4_wz",
      "authorship_tag": "ABX9TyOGtuEUd+sW6c2kq56A8NsS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruey-Day/AI_Assistant/blob/main/AI_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou3MzDUam4Xo",
        "outputId": "a504fb24-7670-4a68-c7f9-2d464a6a6a0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ubuntu.com (185.125.190\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ubuntu.com (185.125.190\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,631 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,830 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,567 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,614 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Fetched 26.9 MB in 4s (7,350 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/Mozilla/Llama-3.2-1B-Instruct-llamafile/resolve/main/Llama-3.2-1B-Instruct.Q6_K.llamafile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uf3EcCNwaa0",
        "outputId": "7cd0bb01-8909-4af5-cef9-a6e66f8f071c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-22 03:50:57--  https://huggingface.co/Mozilla/Llama-3.2-1B-Instruct-llamafile/resolve/main/Llama-3.2-1B-Instruct.Q6_K.llamafile\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.23, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/cd/f8/cdf82c77aab67a8cb9b8806c74f847826ef3656bd3b766306a08646031742ccc/4b20c7f834a5865f848d333dd83478cea7c3250aaad280bc1fd831cc5c960631?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct.Q6_K.llamafile%3B+filename%3D%22Llama-3.2-1B-Instruct.Q6_K.llamafile%22%3B&Expires=1735098657&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNTA5ODY1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2NkL2Y4L2NkZjgyYzc3YWFiNjdhOGNiOWI4ODA2Yzc0Zjg0NzgyNmVmMzY1NmJkM2I3NjYzMDZhMDg2NDYwMzE3NDJjY2MvNGIyMGM3ZjgzNGE1ODY1Zjg0OGQzMzNkZDgzNDc4Y2VhN2MzMjUwYWFhZDI4MGJjMWZkODMxY2M1Yzk2MDYzMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=LWAXEA660exc%7E9UijTK6XsKqXvt3y-LLVN68ACoudu2kQx8qPf-k20hZ0NXLNKG%7EXctjxO-VF83TgJCVMM6YokwYkog1Uwz1i9QY6Y1C9K4mrl%7En0XtN%7E9KNAwjh1TD4jngGV7iSYeXUguRRni7MjQrSDd60KZOCXM%7EuWRc3c1BEk5n9V43WK9mFlxFrOBy2OMN2K44glGmpg9-pQukZnjPKKR0aEJ3Oi7yy6kW-uqteoN9VF8dNaE9fqjMmyj28VhtDwpluvD5x1IYMCgo3x3m%7EXWxpMd6FRShkl%7Eti8rdnUqz4o5LBaq--sPOeGxTSx5uMuI937F74d85By1nb6g__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-12-22 03:50:57--  https://cdn-lfs-us-1.hf.co/repos/cd/f8/cdf82c77aab67a8cb9b8806c74f847826ef3656bd3b766306a08646031742ccc/4b20c7f834a5865f848d333dd83478cea7c3250aaad280bc1fd831cc5c960631?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct.Q6_K.llamafile%3B+filename%3D%22Llama-3.2-1B-Instruct.Q6_K.llamafile%22%3B&Expires=1735098657&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNTA5ODY1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2NkL2Y4L2NkZjgyYzc3YWFiNjdhOGNiOWI4ODA2Yzc0Zjg0NzgyNmVmMzY1NmJkM2I3NjYzMDZhMDg2NDYwMzE3NDJjY2MvNGIyMGM3ZjgzNGE1ODY1Zjg0OGQzMzNkZDgzNDc4Y2VhN2MzMjUwYWFhZDI4MGJjMWZkODMxY2M1Yzk2MDYzMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=LWAXEA660exc%7E9UijTK6XsKqXvt3y-LLVN68ACoudu2kQx8qPf-k20hZ0NXLNKG%7EXctjxO-VF83TgJCVMM6YokwYkog1Uwz1i9QY6Y1C9K4mrl%7En0XtN%7E9KNAwjh1TD4jngGV7iSYeXUguRRni7MjQrSDd60KZOCXM%7EuWRc3c1BEk5n9V43WK9mFlxFrOBy2OMN2K44glGmpg9-pQukZnjPKKR0aEJ3Oi7yy6kW-uqteoN9VF8dNaE9fqjMmyj28VhtDwpluvD5x1IYMCgo3x3m%7EXWxpMd6FRShkl%7Eti8rdnUqz4o5LBaq--sPOeGxTSx5uMuI937F74d85By1nb6g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.97, 18.164.174.98, 18.164.174.19, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1192179873 (1.1G) [binary/octet-stream]\n",
            "Saving to: ‘Llama-3.2-1B-Instruct.Q6_K.llamafile’\n",
            "\n",
            "Llama-3.2-1B-Instru 100%[===================>]   1.11G  40.4MB/s    in 28s     \n",
            "\n",
            "2024-12-22 03:51:25 (40.3 MB/s) - ‘Llama-3.2-1B-Instruct.Q6_K.llamafile’ saved [1192179873/1192179873]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x Llama-3.2-1B-Instruct.Q6_K.llamafile\n",
        "!wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf\n",
        "!chmod +x /usr/bin/ape\n",
        "!sh -c \"echo ':APE:M::MZqFpD::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register\"\n",
        "!sh -c \"echo ':APE-jart:M::jartsr::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1aWWoKlwpr7",
        "outputId": "888ef0b7-a7cf-4f59-b5e6-cf3fad0ab90a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-22 04:06:21--  https://cosmo.zip/pub/cosmos/bin/ape-x86_64.elf\n",
            "Resolving cosmo.zip (cosmo.zip)... 34.136.86.162\n",
            "Connecting to cosmo.zip (cosmo.zip)|34.136.86.162|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9249 (9.0K) [application/octet-stream]\n",
            "Saving to: ‘/usr/bin/ape’\n",
            "\n",
            "/usr/bin/ape        100%[===================>]   9.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-22 04:06:21 (155 MB/s) - ‘/usr/bin/ape’ saved [9249/9249]\n",
            "\n",
            "sh: 1: cannot create /proc/sys/fs/binfmt_misc/register: Directory nonexistent\n",
            "sh: 1: cannot create /proc/sys/fs/binfmt_misc/register: Directory nonexistent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "!ngrok config add-authtoken 2n7KP8ZvSgwsra1Wz28EyqjPp8y_7UWEnj1gVrx5CnzbqeiQ5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN7kJEag0zYU",
        "outputId": "86251ed0-13b0-4eef-eda0-94f8dbdeb310"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8081)  # Expose port 8081\n",
        "print(f\"Public URL: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUEq1T4h0z02",
        "outputId": "64efcb3c-fd62-4a77-fb0e-955f650ff444"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://c6c5-34-16-171-9.ngrok-free.app\" -> \"http://localhost:8081\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./Llama-3.2-1B-Instruct.Q6_K.llamafile --server --nobrowser\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmF4CPnwyJL",
        "outputId": "6836601b-55a0-45d6-af63-638ba906d024"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "note: if you have an AMD or NVIDIA GPU then you need to pass -ngl 9999 to enable GPU offloading\n",
            "{\"build\":1500,\"commit\":\"a30b324\",\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2911,\"msg\":\"build info\",\"tid\":\"12066432\",\"timestamp\":1734840388}\n",
            "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2918,\"msg\":\"system info\",\"n_threads\":1,\"n_threads_batch\":1,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \",\"tid\":\"12066432\",\"timestamp\":1734840388,\"total_threads\":2}\n",
            "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from Llama-3.2-1B-Instruct.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                         general.size_label str              = 1.2B\n",
            "llama_model_loader: - kv   3:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   4:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   5:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   6:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type q6_K:  113 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 16\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 512\n",
            "llm_load_print_meta: n_embd_v_gqa     = 512\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 1.24 B\n",
            "llm_load_print_meta: model size       = 967.00 MiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = n/a\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
            "\u001b[Kllm_load_tensors:        CPU buffer size =   967.00 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   544.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 518\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":502,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"12066432\",\"timestamp\":1734840390}\n",
            "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":514,\"msg\":\"new slot\",\"n_ctx_slot\":8192,\"slot_id\":0,\"tid\":\"12066432\",\"timestamp\":1734840390}\n",
            "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":3131,\"msg\":\"model loaded\",\"tid\":\"12066432\",\"timestamp\":1734840390}\n",
            "\n",
            "llama server listening at http://127.0.0.1:8081\n",
            "\n",
            "In the sandboxing block!\n",
            "{\"function\":\"server_cli\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3269,\"msg\":\"HTTP server listening\",\"port\":\"8081\",\"tid\":\"12066432\",\"timestamp\":1734840390,\"url_prefix\":\"\"}\n",
            "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1672,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"12066432\",\"timestamp\":1734840390}\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}